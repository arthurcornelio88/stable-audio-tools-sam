{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 3 - Downloading and renaming the audio files (.mp3)"]},{"cell_type":"markdown","metadata":{"id":"wkjvZM-PUBhF"},"source":["## 1. Load genre dataframe (.csv)\n","### a) Load .env variables\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from dotenv import load_dotenv\n","\n","load_dotenv()\n","\n","GITHUB_PROFILE_NAME = os.getenv('GITHUB_PROFILE_NAME')"]},{"cell_type":"markdown","metadata":{},"source":["### b) Import dataframe\n","- ⚠️ change line 17 to set the .csv filename"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["import os\n","from pathlib import Path\n","import pandas as pd\n","\n","def get_csv_path(filename, git_name):\n","    \"\"\"\n","    Constructs the path to a CSV file based on the provided filename and GitHub profile name.\n","    Assumes a specific project structure where the CSV files are located in a 'dataframes'\n","    subdirectory within the 'sam_files' directory of a project named after the GitHub profile name.\n","    \"\"\"\n","\n","    # Capture the root directory and construct the path\n","    root_dir = Path.cwd().parent.parent.parent.parent.parent  # Adjust if needed based on your actual structure\n","    base_path = root_dir / f\"{git_name}/stable-audio-tools-sam/sam_files/dataframes\"\n","\n","    # Combine the base path with the filename\n","    csv_path = base_path / filename\n","    return csv_path\n","\n","# Use the function to get the path\n","filename = \"test_df_notebook.csv\"\n","csv_path = get_csv_path(filename, os.getenv('GITHUB_PROFILE_NAME'))  # Assuming you have GITHUB_PROFILE_NAME set in your environment\n","\n","# Load the DataFrame\n","df = pd.read_csv(csv_path)"]},{"cell_type":"markdown","metadata":{"id":"glyS9-a6UJPU"},"source":["## 2. Create .json files"]},{"cell_type":"markdown","metadata":{},"source":["### a) Script to process text from .csv to JSON file"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":295,"status":"ok","timestamp":1722974686997,"user":{"displayName":"Arthur Cornélio","userId":"06652063742078674773"},"user_tz":-120},"id":"k5nlfdDnyFEr","outputId":"153845a8-95b3-4f00-ce50-4a4ca27eac83"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to\n","[nltk_data]     /home/arthurcornelio/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["'''\n","    This script processes text data from a CSV file, extracting common phrases\n","        and keywords from specified columns.\n","    It then generates JSON files for each row, using the keywords as prompts,\n","        with filenames sanitized for filesystem compatibility.\n","    The script handles bigrams, trigrams, and individual words, ensuring no\n","        overlap between the different types of keywords.\n","'''\n","\n","import pandas as pd\n","import json\n","import os\n","import re\n","from collections import Counter\n","from nltk.corpus import wordnet as wn\n","from sklearn.feature_extraction.text import CountVectorizer\n","import nltk\n","\n","# Download the 'wordnet' resource\n","nltk.download('wordnet')\n","\n","# Initialize the lemmatizer\n","lemmatizer = nltk.WordNetLemmatizer()\n","\n","def extract_ngrams(text, n):\n","    \"\"\"\n","    Extract n-grams from a given text.\n","\n","    Parameters:\n","    - text: A string of text.\n","    - n: The number of words in the n-gram.\n","\n","    Returns:\n","    - A list of n-grams.\n","    \"\"\"\n","    vectorizer = CountVectorizer(ngram_range=(n, n), stop_words=None)\n","    analyzer = vectorizer.build_analyzer()\n","    return analyzer(text)\n","\n","def is_meaningful_phrase(phrase):\n","    \"\"\"\n","    Check if a phrase is meaningful by verifying its components.\n","\n","    Parameters:\n","    - phrase: A string representing the phrase.\n","\n","    Returns:\n","    - True if the phrase is meaningful, False otherwise.\n","    \"\"\"\n","    words = phrase.split()\n","    # Check if the individual words are meaningful and make sense together\n","    for word in words:\n","        if not wn.synsets(word):\n","            return False\n","    return True\n","\n","def find_common_phrases(df, columns, min_count=5):\n","    \"\"\"\n","    Find common phrases (bigrams and trigrams) in the specified columns of the dataframe.\n","\n","    Parameters:\n","    - df: The input dataframe.\n","    - columns: List of column names to process.\n","    - min_count: Minimum count to consider a phrase as common.\n","\n","    Returns:\n","    - A set of common phrases.\n","    \"\"\"\n","    phrases = []\n","    for col in columns:\n","        for text in df[col].dropna():\n","            phrases.extend(extract_ngrams(text.lower(), 2))  # Bigrams\n","            phrases.extend(extract_ngrams(text.lower(), 3))  # Trigrams\n","\n","    phrase_counts = Counter(phrases)\n","    common_phrases = {phrase for phrase, count in phrase_counts.items() if count >= min_count and is_meaningful_phrase(phrase)}\n","    return common_phrases\n","\n","def process_keywords(row, columns, common_phrases):\n","    \"\"\"\n","    Concatenate and process keywords from specified columns, splitting by newline characters.\n","\n","    Parameters:\n","    - row: A pandas Series representing a row in the dataframe.\n","    - columns: List of column names to process.\n","    - common_phrases: Set of common phrases to preserve as composite keywords.\n","\n","    Returns:\n","    - A string containing concatenated keywords.\n","    \"\"\"\n","    exclude_keywords = {'main', 'title'}\n","    keywords = []\n","    composite_keywords = []\n","\n","    for col in columns:\n","        if pd.notna(row[col]):\n","            words = row[col].replace('\\n', ' ').lower().split()\n","            words = [word for word in words if word not in exclude_keywords and len(word) > 2]  # Exclude single chars and common keywords\n","            if words:\n","                # Identify and preserve composite keywords\n","                composite = {phrase for phrase in common_phrases if phrase in row[col].lower()}\n","                composite_keywords.extend(composite)\n","                for composite_kw in composite:\n","                    if composite_kw not in keywords:\n","                        keywords.append(composite_kw)\n","\n","                # Add remaining single words\n","                for word in words:\n","                    if word not in keywords:\n","                        keywords.append(word)\n","\n","    # Create a list of unique keywords and phrases\n","    unique_keywords = []\n","    for kw in composite_keywords:\n","        if all(kw not in other_kw for other_kw in unique_keywords):\n","            unique_keywords.append(kw)\n","\n","    unique_keywords.extend([kw for kw in keywords if kw not in unique_keywords])\n","\n","    # Select keywords by type, ensuring no overlap\n","    three_word_phrases = [kw for kw in unique_keywords if len(kw.split()) == 3][:4]\n","    two_word_phrases = [kw for kw in unique_keywords if len(kw.split()) == 2 and all(kw not in three_word for three_word in three_word_phrases)][:3]\n","    single_words = [kw for kw in unique_keywords if len(kw.split()) == 1 and all(kw not in two_word for two_word in two_word_phrases)][:8]\n","\n","    # Combine all keywords and join into a single string\n","    final_keywords = three_word_phrases + two_word_phrases + single_words\n","\n","    return ', '.join(final_keywords)\n","\n","def sanitize_filename(name):\n","    \"\"\"\n","    Sanitize the filename to remove or replace invalid characters.\n","\n","    Parameters:\n","    - name: The original filename string.\n","\n","    Returns:\n","    - A sanitized filename string.\n","    \"\"\"\n","    # Replace invalid characters with an underscore\n","    return re.sub(r'[<>:\"/\\\\|?*]', '_', name)\n","\n","def create_json_files(csv_path, output_dir):\n","    \"\"\"\n","    Create JSON files from a CSV dataset.\n","\n","    Parameters:\n","    - csv_path: Path to the input CSV file.\n","    - output_dir: Directory where JSON files will be saved.\n","    \"\"\"\n","    # Read CSV file\n","    df = pd.read_csv(csv_path)\n","\n","    # Columns to process\n","    columns = ['Genre', 'Mood', 'Movement', 'Theme', 'Other keywords', 'Other keywords.1']\n","\n","    # Find common phrases in the dataset\n","    common_phrases = find_common_phrases(df, columns)\n","\n","    # Create output directory if it doesn't exist\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","\n","    # Process each row and create JSON files\n","    for index, row in df.iterrows():\n","        prompt = process_keywords(row, columns, common_phrases)\n","\n","        # Use the 'Title' column value as the filename, sanitize it for filesystem\n","        title = row.get('Title', f'file_{index+1}').strip()\n","        sanitized_title = sanitize_filename(title)\n","        file_name = f\"{index+1}_{sanitized_title}.json\"\n","        file_path = os.path.join(output_dir, file_name)\n","\n","        # Prepare the data to be saved in JSON\n","        data = {\"prompt\": prompt}\n","\n","        # Write JSON file\n","        with open(file_path, 'w') as f:\n","            json.dump(data, f, indent=4)\n","\n","        print(f\"Created {file_path}\")"]},{"cell_type":"markdown","metadata":{},"source":["### b) Define output folder name\n","\n","- ⚠️ Set line 19\n","- Verify its output. It's your output_dir, it needs to be in this path:\n","  - sam_files/json/json_{genre}"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"bvOVtcZ4ydHN"},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/arthurcornelio/code/None/stable-audio-tools-sam/sam_files/json/json_test\n"]}],"source":["import os\n","from pathlib import Path\n","\n","def get_output_dir(genre):\n","    \"\"\"\n","    Gets the output directory based on the user's home directory and GitHub profile name.\n","    \"\"\"\n","\n","    # Automatically get the user's home directory\n","    home_dir = os.path.expanduser('~')\n","\n","    # Capture the GitHub profile name from the environment variable\n","    github_profile_name = os.getenv('GITHUB_PROFILE_NAME')\n","\n","    # Construct the output directory path\n","    return os.path.join(home_dir, f'code/{github_profile_name}/stable-audio-tools-sam/sam_files/json/json_{genre}')\n","\n","# Usage example\n","genre_folder = \"test\"\n","!echo \"genre_folder={genre_folder}\" >> .env\n","output_dir = get_output_dir(genre_folder)\n","print(output_dir)"]},{"cell_type":"markdown","metadata":{},"source":["### c) Run the cell below to create the json files"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["create_json_files(csv_path, output_dir)"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Keep going to the next notebook !"]},{"cell_type":"markdown","metadata":{"id":"b-ZTAGfBT5Sk"},"source":["# 4. (Optional) Verify first row from table"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1722962002388,"user":{"displayName":"Arthur Cornélio","userId":"06652063742078674773"},"user_tz":-120},"id":"G_04s8-80idr","outputId":"f377b896-7c6f-4ae7-d0d7-7732d754a040"},"outputs":[{"name":"stdout","output_type":"stream","text":["Output saved to /home/arthurcornelio/code/arthurcornelio88/stable-audio-tools-sam/sam_files/notebooks/dataset for fine-tuning/first_row_output.txt\n"]}],"source":["import os\n","from pathlib import Path\n","\n","# Columns to print\n","columns = ['Genre', 'Mood', 'Movement', 'Theme', 'Other keywords', 'Other keywords.1']\n","\n","# Print the specified columns for the first row\n","first_row = df.loc[0, columns]\n","\n","# Replace newline characters with spaces\n","first_row = first_row.apply(lambda x: x.replace('\\n', ' ') if isinstance(x, str) else x)\n","\n","# Determine the output path (always in the current directory)\n","current_dir = Path.cwd()\n","output_path = os.path.join(current_dir, 'first_row_output.txt')\n","\n","# Save the specified columns for the first row to a file\n","with open(output_path, 'w') as f:\n","    f.write(\"First row values for specified columns:\\n\")\n","    for col in columns:\n","        # Limit the length of the printed content to avoid very long lines\n","        value = first_row[col]\n","        if isinstance(value, str) and len(value) > 100:\n","            value = value[:100] + '...'\n","        f.write(f\"{col}: {value}\\n\")\n","\n","print(f\"Output saved to {output_path}\")\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPxet96r0TyAJyoHAG971id","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}
