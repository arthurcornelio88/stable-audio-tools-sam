{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 5 - Uploading to S.A.M. bucket"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Load .env variables\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# load .env variables\n","from dotenv import load_dotenv\n","import os\n","\n","load_dotenv()\n","\n","GITHUB_PROFILE_NAME = os.getenv('GITHUB_PROFILE_NAME')\n","genre = os.getenv('genre')\n","file_count = os.getenv('file_count')\n","final_folder_name = os.getenv('final_folder_name')"]},{"cell_type":"markdown","metadata":{},"source":["## 2 Organizing dataframe files"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# reorganize dataframes for uploading\n","!python scripts/reorganize_df_files.py"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Uploading to S3 Bucket"]},{"cell_type":"markdown","metadata":{},"source":["### 3.1. You need to be in :\n","  - `../stable-audio-tools-sam/sam_files`"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/arthurcornelio/code/arthurcornelio88/stable-audio-tools-sam/sam_files/notebooks\n","/home/arthurcornelio/code/arthurcornelio88/stable-audio-tools-sam/sam_files\n"]}],"source":["# run this cell (path move)\n","%cd ..\n","%cd .."]},{"cell_type":"markdown","metadata":{},"source":["### 3.3 Upload your AWS Credentials\n","- Put the \"rootkey.csv\" in /sam-files\n","- - ../Modèles/RunPod-arthur/runpod-repo/model_v2\n","- Run the cell. The values will go to the variables, the \"rootkey.csv\" will be deleted."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# upload AWS Credentials\n","import pandas as pd\n","import os\n","\n","aws_key = pd.read_csv(\"rootkey.csv\")\n","\n","AWS_ACCESS_KEY_ID = aws_key['Access key ID'][0]\n","AWS_SECRET_ACCESS_KEY = aws_key['Secret access key'][0]\n","\n","os.remove(\"rootkey.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["### 3.3  Define your BUCKET_NAME"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["BUCKET_NAME = 'runpod-sam-model'"]},{"cell_type":"markdown","metadata":{},"source":["### 3.2. Creating .tar file for uploading"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["5_test_classical_6/\n","5_test_classical_6/4_majestic-voyage-200663.json\n","5_test_classical_6/4_majestic-voyage-200663.mp3\n","5_test_classical_6/5_drive-to-triumph-188794.mp3\n","5_test_classical_6/5_drive-to-triumph-188794.json\n","5_test_classical_6/3_science-documentary-169621.mp3\n","5_test_classical_6/2_risk-136788.json\n","5_test_classical_6/1_inspiring-cinematic-ambient-116199.mp3\n","5_test_classical_6/2_risk-136788.mp3\n","5_test_classical_6/1_inspiring-cinematic-ambient-116199.json\n","5_test_classical_6/3_science-documentary-169621.json\n"]},{"data":{"text/plain":["0"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# ERASE THIS\n","import os\n","\n","tar_code = f'tar -cvf 5_test_classical_json6.tar 5_test_classical_6'\n","os.system(tar_code)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# create .tar file\n","import os\n","\n","tar_code = f'tar -cvf {final_folder_name}.tar audio_files/by_genre/{final_folder_name}'\n","os.system(tar_code)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# if needed, install boto3\n","%pip install boto3"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# upload bucket in amazon s3\n","import boto3\n","import os\n","import logging\n","from boto3.s3.transfer import S3Transfer, TransferConfig\n","\n","load_dotenv()\n","TIMESTAMP_DF = os.getenv('TIMESTAMP_DF')\n","\n","# Set up logging to a file\n","logging.basicConfig(filename='upload_log.txt', level=logging.DEBUG,\n","                    format='%(asctime)s - %(levelname)s - %(message)s')\n","\n","# Disable console logging from Boto3\n","logging.getLogger('boto3').setLevel(logging.CRITICAL)  # Set to CRITICAL to suppress all Boto3 messages\n","logging.getLogger('botocore').setLevel(logging.CRITICAL)\n","logging.getLogger('s3transfer').setLevel(logging.CRITICAL)\n","\n","# Initialize S3 client\n","print(\"Establishing connection to S3... \", end=\"\")\n","s3 = boto3.client('s3',\n","                  aws_access_key_id=AWS_ACCESS_KEY_ID,\n","                  aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n","print(\"Connected!\")\n","\n","# Files to upload\n","tar_to_upload = f'{final_folder_name}.tar'\n","folder_to_upload = f'dataframes_{TIMESTAMP_DF}'\n","\n","# Upload the folder first\n","total_files = sum([len(files) for root, dirs, files in os.walk(folder_to_upload)])\n","uploaded_files = 0\n","\n","print(f\"Uploading {total_files} files from '{folder_to_upload}' to {BUCKET_NAME}/{folder_to_upload}:\")\n","for root, dirs, files in os.walk(folder_to_upload):\n","    for file in files:\n","        file_path = os.path.join(root, file)\n","\n","        # Include the folder_to_upload in the S3 key\n","        s3_key = os.path.join(folder_to_upload, os.path.relpath(file_path, folder_to_upload))\n","\n","        print(f\"  Uploading {file_path}... \", end=\"\")\n","        s3.upload_file(file_path, BUCKET_NAME, s3_key)\n","        print(\"Done!\")\n","\n","        uploaded_files += 1\n","        progress = (uploaded_files / total_files) * 100\n","        print(f\"  Progress: {progress:.1f}%\")\n","\n","print(\"Folder upload complete!\")\n","\n","# Upload the zipped file using multipart upload with S3Transfer\n","print(f\"\\nUploading {tar_to_upload} to {BUCKET_NAME}...\")\n","\n","# Configure multipart upload and transfer\n","config = TransferConfig(multipart_threshold=1024 * 25,\n","                        max_concurrency=10,\n","                        multipart_chunksize=1024 * 25,\n","                        use_threads=True)\n","transfer = S3Transfer(s3, config)\n","\n","# Use a mutable object (list) to store total_transferred\n","total_transferred = [0]\n","\n","# Define a progress callback function\n","def progress_callback(bytes_transferred):\n","    total_transferred[0] += bytes_transferred\n","    percent = total_transferred[0] / file_size * 100\n","    bar = '█' * int(percent / 2) + '-' * int(50 - percent / 2)\n","    print(f\"\\r|{bar}| {percent:.1f}%\", end=\"\")\n","\n","# Get the file size before uploading\n","file_size = os.path.getsize(tar_to_upload)\n","\n","# Perform the multipart upload with progress callback\n","transfer.upload_file(tar_to_upload, BUCKET_NAME, tar_to_upload, callback=progress_callback)\n","\n","print(\"\\nTar file upload complete!\")\n","\n","print(\"All uploads complete!\")"]},{"cell_type":"markdown","metadata":{},"source":["### 3.3. Verify if uploading operation is successful"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# verify upload consistency\n","import boto3\n","\n","# Initialize S3 client (assuming you have your credentials set up)\n","s3 = boto3.client('s3',\n","                  aws_access_key_id=AWS_ACCESS_KEY_ID,\n","                  aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n","\n","# List objects in your bucket\n","response = s3.list_objects_v2(Bucket=BUCKET_NAME)\n","\n","# Extract object keys (filenames)\n","uploaded_objects = [obj['Key'] for obj in response.get('Contents', [])]\n","\n","# Files and folders you expected to upload\n","expected_uploads = [tar_to_upload] + [os.path.join(root, file) for root, dirs, files in os.walk(folder_to_upload) for file in files]\n","\n","# Convert expected uploads to S3 keys (relative paths)\n","expected_uploads = [os.path.relpath(path, folder_to_upload) if path.startswith(folder_to_upload) else path for path in expected_uploads]\n","\n","# Compare uploaded objects with expected uploads\n","missing_uploads = set(expected_uploads) - set(uploaded_objects)\n","extra_uploads = set(uploaded_objects) - set(expected_uploads)\n","\n","if missing_uploads or extra_uploads:\n","    if missing_uploads:\n","        print(\"The following files/folders were not uploaded:\", missing_uploads)\n","    if extra_uploads:\n","        print(\"The following unexpected files/folders were found in the bucket:\", extra_uploads)\n","else:\n","    print(\"All expected files/folders were uploaded successfully, and no unexpected uploads were found!\")"]},{"cell_type":"markdown","metadata":{},"source":["## Delete final operation folders "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# delete operational folders and files\n","\n","# Delete the folders and their contents\n","!rm -rf \"notebooks/dataset for fine-tuning/myenv\"\n","\n","!rm -rf dataframes/* dataframes_${TIMESTAMP_DF}/* audio_files/final_backup/* audio_files/by_genre/*  json/*\n","\n","# Delete .tar file\n","!rm ${final_folder_name}.tar\n","#!rm 498_classical_files_2024-08-28_01-13-37.tar"]},{"cell_type":"markdown","metadata":{},"source":["# All done, bravo ! (by Arthur Cornélio, 28th August 2024)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPxet96r0TyAJyoHAG971id","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}
