{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 5 - Uploading to S.A.M. bucket"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Load .env variables\n","\n"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["from dotenv import load_dotenv\n","import os\n","\n","load_dotenv()\n","\n","GITHUB_PROFILE_NAME = os.getenv('GITHUB_PROFILE_NAME')\n","genre_folder = os.getenv('genre_folder')\n","file_count = os.getenv('file_count')"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Move manually your .JSON credential\n","  - Put it in /sam_files\n","  - Open the .env for sam_files (sam_files/notebooks/dataset for fine-tuning/.env)\n","  - insert this line :\n","    - `gdrive_api_json_path={path-for-json-credential.json}`"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Uploading to Google Cloud Bucket"]},{"cell_type":"markdown","metadata":{},"source":["### 3.1. You need to be in :\n","  - `../stable-audio-tools-sam/sam_files`"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/arthurcornelio/code/arthurcornelio88/stable-audio-tools-sam/sam_files/notebooks\n","/home/arthurcornelio/code/arthurcornelio88/stable-audio-tools-sam/sam_files\n"]}],"source":["%cd ..\n","%cd .."]},{"cell_type":"markdown","metadata":{},"source":["### 3.2. Copying folders to bucket"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%bash\n","\n","# Replace with your actual bucket name\n","bucket_name=\"sam-dataset\"\n","\n","# Get the current timestamp\n","timestamp=$(date +\"%Y-%m-%d_%H-%M-%S\")\n","\n","# Create the folder name within the bucket\n","folder_name=\"${file_count}_${genre_folder}_files_${timestamp}\"\n","\n","#capture variables\n","echo \"bucket_name=$bucket_name\" >> \"notebooks/dataset for fine-tuning/.env\"\n","echo \"timestamp=$timestamp\" >> \"notebooks/dataset for fine-tuning/.env\"\n","echo \"folder_name=$folder_name\" >> \"notebooks/dataset for fine-tuning/.env\"\n","\n","# List of source folders you want to upload\n","source_folders=(\"json\" \"dataframes\" \"audio_files\")\n","\n","# Upload each folder to the bucket\n","for source_folder in \"${source_folders[@]}\"\n","do\n","    gsutil -m cp -r -L \"upload_log_${timestamp}.txt\" \"$source_folder\" \"gs://$bucket_name/$folder_name/\"\n","done\n"]},{"cell_type":"markdown","metadata":{},"source":["### 3.3. Verify if uploading operation is successful"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from google.cloud import storage\n","import os\n","from dotenv import load_dotenv\n","\n","# Load environment variables from .env file\n","load_dotenv(dotenv_path='notebooks/dataset for fine-tuning/.env')\n","\n","# Access the variables from the .env file\n","bucket_name = os.getenv('bucket_name')\n","folder_name = os.getenv('folder_name')\n","\n","def verify_upload(bucket_name, folder_name, source_folders):\n","    \"\"\"Verifies if the specified folder and its subfolders exist in the given GCS bucket.\"\"\"\n","\n","    # Get the path to your service account credentials JSON file from the environment variable\n","    credentials_path = os.getenv('gdrive_api_json_path')\n","\n","    # Explicitly create the storage client using the service account credentials\n","    storage_client = storage.Client.from_service_account_json(credentials_path)\n","\n","    bucket = storage_client.bucket(bucket_name)\n","\n","    # Check if the main folder exists\n","    blobs = list(bucket.list_blobs(prefix=folder_name + '/'))\n","    if not blobs:\n","        print(f\"Upload failed or incomplete. Main folder '{folder_name}' not found in bucket '{bucket_name}'.\")\n","        return  # Stop further checks if the main folder is missing\n","\n","    # Check for the existence of each subfolder within the main folder\n","    for source_folder in source_folders:\n","        blobs = list(bucket.list_blobs(prefix=f\"{folder_name}/{source_folder}/\"))\n","\n","        if blobs:\n","            print(f\"Upload successful! Subfolder '{source_folder}' and its contents found in bucket '{bucket_name}' under '{folder_name}'.\")\n","            # (Optional) You can iterate through 'blobs' to list individual files if needed\n","            # for blob in blobs:\n","            #   print(f\"  - {blob.name}\")\n","        else:\n","            print(f\"Upload failed or incomplete. Subfolder '{source_folder}' not found in bucket '{bucket_name}' under '{folder_name}'.\")\n","\n","# List of source folders you want to upload\n","source_folders = [\n","    'json',\n","    'dataframes/checked',\n","    'dataframes/filtered_by_genre',\n","    'audio_files/by_genre',\n","    'audio_files/final_backup',\n","]\n","\n","verify_upload(bucket_name, folder_name, source_folders)\n"]},{"cell_type":"markdown","metadata":{},"source":["## 4. Delete final operation folders "]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["%%bash\n","\n","# Delete the folders and their contents\n","rm -rf audio_files/final_backup audio_files/by_genre json/*"]},{"cell_type":"markdown","metadata":{},"source":["# All done, bravo ! (by Arthur Corn√©lio, 12th August 2024)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPxet96r0TyAJyoHAG971id","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}
